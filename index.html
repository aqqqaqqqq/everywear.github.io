<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>EveryWear</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <!-- <script defer src="static/js/fontawesome.all.min.js"></script> -->
  <script src="https://kit.fontawesome.com/edbfd41333.js" crossorigin="anonymous"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

<section class="section head-section">
  <div class="container is-max-desktop">
    <div class="is-centered">
      <div class="has-text-centered">
        <h2 class="title is-2">Human Motion Estimation with Everyday Wearables</h2>
        <!-- <p class="has-text-danger publisher">CVPR 2024, Highlight</p> -->

        <div class="is-size-5">
          <span class="author-block">
            <a href="/" target="_blank">Siqi Zhu</a><sup>1*</sup>,</span>
          <span class="author-block">
            <a href="/" target="_blank">Yixuan Li</a><sup>1*</sup>,</span>
          <span class="author-block">
            <a href="/" target="_blank">Junfu Li</a><sup>1*</sup>,</span>
          <span class="author-block">
            <a href="/" target="_blank">Qi Wu</a><sup>1*</sup>,</span>
          <br>
          <span class="author-block">
            <a href="https://silvester.wang/" target="_blank">Zan Wang</a><sup>1*</sup>,</span>
          <span class="author-block">
            <a href="/" target="_blank">Haozhe Ma</a><sup>2</sup>,</span>
          <span class="author-block">
            <a href="https://liangwei-bit.github.io/web/" target="_blank">Wei Liang</a><sup>1,3‚úâÔ∏è</sup>,</span>
        </div>

        <div class="is-size-6">
          <span class="author-block">
            <sup>1</sup>School of Computer Science & Technology, Beijing Institute of Technology<br>
            <sup>2</sup><br>
            <sup>3</sup>Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing
          </span>
          <br>
          <span class="eql-cntrb">
            <sup>*</sup>indicates equal contribution &nbsp; <sup>‚úâÔ∏è</sup>indicates corresponding author</span>
        </div>

        <div class="link-block">
          <span>
            <a href="/" target="_blank"
              class="external-link button is-normal is-rounded is-outlined is-info">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>

          <span>
            <a href="/" target="_blank"
              class="external-link button is-normal is-rounded is-outlined is-danger">
              <span class="icon">
                <i class="fas fa-file-pdf"></i>
              </span>
              <span>Paper</span>
            </a>
          </span>

          <span>
            <a href="/" target="_blank"
              class="external-link button is-normal is-rounded is-outlined is-primary">
              <span class="icon">
                <i class="fas fa-file-pdf"></i>
              </span>
              <span>Supplementary</span>
            </a>
          </span>

          <span>
            <a href="/" target="_blank"
              class="external-link button is-normal is-rounded is-outlined is-link">
              <span class="icon">
                <i class="fas fa-video"></i>
              </span>
              <span>Video</span>
            </a>
          </span>

          <span>
            <a href="/" target="_blank"
              class="external-link button is-normal is-rounded is-outlined is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section content-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-10">
        <video  autoplay controls muted loop height="100%">
          <source src="/" type="video/mp4">
        </video>
        <h6 class="subtitle is-6">
          üèÉ‚Äç‚ôÄÔ∏è We introduce a novel two-stage framework that <strong>employs scene affordance as an intermediate representation</strong>, effectively linking 3D scene grounding and conditional motion generation.
        </h6>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h3 class="title is-3">ü•∞ Abstract</h3>
        <div class="has-text-justified">
          <p>
            While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, 
            expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present <strong>EveryWear<strong>, 
            a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped 
            with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce </strong>Ego-Elec</strong>, a 9-hour 
            real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion 
            capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates 
            visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model 
            effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its 
            effectiveness for practical full-body motion estimation.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-12">
        <h3 class="title has-text-centered is-3">ü•∏ BibTeX</h3>
        <div class="bibtex-block">
          <pre><code>@inproceedings{zhu2025human,
  title={Human Motion Estimation with Everyday Wearables},
  author={Zhu, Siqi and Li, Yixuan and Li, Junfu and Wu, Qi and Wang, Zan and Ma, Haozhe and Liang, Wei},
  booktitle={},
  year={2025}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


</body>
  
</html>
